{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = 2048\n",
    "semantic_dim = 500\n",
    "# latent_dim = 120\n",
    "latent_dim = 50\n",
    "n_critic = 5\n",
    "\n",
    "epochs=50000\n",
    "batch_size=32\n",
    "sample_interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network\n",
    "z = Input(shape=(latent_dim,),name='z_input')\n",
    "semantics = Input(shape=(semantic_dim,),name='semantics_input')\n",
    "\n",
    "merged_layer = Concatenate()([z,semantics])\n",
    "generator = Dense(2048, activation=\"relu\")(merged_layer)\n",
    "\n",
    "generator = Dense(2048, activation=\"relu\")(generator)\n",
    "# generator = Activation(\"tanh\")(generator)\n",
    "\n",
    "generator = Model(inputs=[z, semantics], outputs=generator, name='generator')\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "z = Input(shape=(latent_dim,),name='z_input')\n",
    "img = Input(shape=(img_dim,),name='img_input')\n",
    "semantics = Input(shape=(semantic_dim,),name='semantics_input')\n",
    "# d_in = concatenate([z, img, semantics])\n",
    "merged_layer = Concatenate()([z, img, semantics])\n",
    "\n",
    "discriminator = Dense(4096)(merged_layer)\n",
    "discriminator = LeakyReLU(alpha=0.2)(discriminator)\n",
    "discriminator = Dropout(0.25)(discriminator)\n",
    "\n",
    "\n",
    "discriminator = Dense(2048)(discriminator)\n",
    "discriminator = LeakyReLU(alpha=0.2)(discriminator)\n",
    "discriminator = Dropout(0.25)(discriminator)\n",
    "\n",
    "discriminator = Dense(1)(discriminator)\n",
    "\n",
    "discriminator = Model(inputs=[z, img, semantics], outputs=discriminator, name='discriminator')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "#         alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        alpha = K.random_uniform((32, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 惩罚函数\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "#它取的是两个图像差异的均值。这种损失函数可以改善生成对抗网络的收敛性。\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "# 均方差\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the discriminator\n",
    "#-------------------------------\n",
    "# Freeze generator's layers while training discriminator\n",
    "generator.trainable = False\n",
    "real_img = Input(shape=(img_dim,),name='real_img')\n",
    "z = Input(shape=(latent_dim,),name='z_input')\n",
    "semantics = Input(shape=(semantic_dim,),name='semantics_input')\n",
    "\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = generator([z, semantics])\n",
    "\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = discriminator([z,fake_img, semantics])\n",
    "valid = discriminator([z,real_img, semantics])\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = discriminator([z,interpolated_img, semantics])\n",
    "\n",
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                  averaged_samples=interpolated_img)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "discriminator_model = Model(inputs=[real_img, z, semantics],\n",
    "                    outputs=[valid, fake, validity_interpolated])\n",
    "\n",
    "\n",
    "optimizer = RMSprop(lr=0.000001)\n",
    "\n",
    "\n",
    "discriminator_model.compile(loss=[wasserstein_loss,\n",
    "                                      wasserstein_loss,\n",
    "                                      partial_gp_loss],\n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])\n",
    "\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------\n",
    "\n",
    "# For the generator we freeze the discriminator's layers\n",
    "discriminator.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z = Input(shape=(latent_dim,),name='z_input')\n",
    "semantics = Input(shape=(semantic_dim,),name='semantics_input')\n",
    "# Generate images based of noise\n",
    "img = generator([z, semantics])\n",
    "# Discriminator determines validity\n",
    "valid = discriminator([z, img, semantics])\n",
    "# Defines generator model\n",
    "generator_model = Model([z, semantics], valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *  \n",
    "import operator\n",
    "def classify(inX,dataSet,labels,k):\n",
    "    # 获取维度\n",
    "    dataSetSize=dataSet.shape[0]   # 训练数据集数量\n",
    "\n",
    "    diffMat=tile(inX,(dataSetSize,1))-dataSet  # 测试样本的各维度的差值\n",
    "\n",
    "    sqDiffMat=diffMat**2  # 平方计算\n",
    "\n",
    "    sqDistance=sqDiffMat.sum(axis=1)  # 输出每行的值\n",
    "\n",
    "    distances=sqDistance**0.5   # 开方计算\n",
    "\n",
    "    sortedDistances=distances.argsort()   # 排序 按距离从小到大 输出索引\n",
    "\n",
    "    classCount={}\n",
    "    for i in range(k):\n",
    "#         print(sortedDistances[i])\n",
    "        voteIlabel=labels[sortedDistances[i]]\n",
    "        classCount[voteIlabel]=classCount.get(voteIlabel,0)+1.0\n",
    "    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def accuracy_train(x,y,z):\n",
    "    group = z\n",
    "    labels= [0,1,2]\n",
    "\n",
    "    num=0\n",
    "    y_pred =[]\n",
    "\n",
    "    for i in range(6000):\n",
    "        res=classify(x[i],group,labels,1)\n",
    "        y_pred.append(res)\n",
    "        \n",
    "        if res == y[i]:\n",
    "             num = num+1\n",
    "\n",
    "    accuracy = num/6000\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def accuracy_test1(x,y,z):\n",
    "    group = z\n",
    "    labels= [0,1,2]\n",
    "\n",
    "    num=0\n",
    "    y_pred =[]\n",
    "\n",
    "    for i in range(6000):\n",
    "        res=classify(x[i],group,labels,1)\n",
    "        y_pred.append(res)\n",
    "        \n",
    "        if res == y[i]:\n",
    "             num = num+1\n",
    "\n",
    "    accuracy = num/6000\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def accuracy_test2(x,y,z):\n",
    "    group = z\n",
    "    labels= [0,1,2,3]\n",
    "\n",
    "    num=0\n",
    "    y_pred =[]\n",
    "\n",
    "    for i in range(8000):\n",
    "        res=classify(x[i],group,labels,1)\n",
    "        y_pred.append(res)\n",
    "        \n",
    "        if res == y[i]:\n",
    "             num = num+1\n",
    "\n",
    "    accuracy = num/8000\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./datas/features/feature_train.npy')\n",
    "y_train = np.load('./datas/train/y_train.npy')\n",
    "\n",
    "x_test = np.load('./datas/features/feature_test.npy')\n",
    "y_test = np.load('./datas/test/y_test.npy')\n",
    "\n",
    "semantic_train = np.load('./datas/semantic_test/semantic_train.npy')\n",
    "semantic_test = np.load('./datas/semantic_test/semantic_test.npy')\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "print(semantic_train.shape,semantic_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_acc_test1 = 0\n",
    "flag_acc_test2 = 0\n",
    "\n",
    "valid = -np.ones((batch_size, 1))\n",
    "fake =  np.ones((batch_size, 1))\n",
    "\n",
    "dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for _ in range(n_critic):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        imgs = x_train[idx]\n",
    "        semantics = semantic_train[idx]\n",
    "        # Sample generator input\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        # Train the discriminator\n",
    "        d_loss = discriminator_model.train_on_batch([imgs, noise, semantics],\n",
    "                                                        [valid, fake, dummy])\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generatorhttp://localhost:8888/notebooks/lk_model/WGANGP.ipynb#\n",
    "    # ---------------------\n",
    "\n",
    "    g_loss = generator_model.train_on_batch([noise,semantics], valid)\n",
    "    \n",
    "    mean_squared_error_loss = mean_squared_error(imgs,generator.predict([noise,semantics]))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % sample_interval == 0:\n",
    "        samples = 7\n",
    "        noise = np.random.normal(0, 1, (samples, latent_dim))\n",
    "        semantics_all = np.load('./datas/semantic_test/semantic_all.npy')\n",
    "        fake_img = generator.predict([noise, semantics_all])\n",
    "        z1 = fake_img[0:3]\n",
    "        z2 = fake_img[3:6]\n",
    "        z3 = fake_img[3:7]\n",
    "        \n",
    "        acc_train= accuracy_train(x_train,y_train,z1)\n",
    "        acc_test1 = accuracy_test1(x_test,y_test,z2)\n",
    "        acc_test2 = accuracy_test2(x_test,y_test,z3)\n",
    "        \n",
    "        if(flag_acc_test1 < acc_test1):\n",
    "            flag_acc_test1 = acc_test1\n",
    "        \n",
    "        if(flag_acc_test2 < acc_test2):\n",
    "            flag_acc_test2 = acc_test2\n",
    "        \n",
    "        \n",
    "        print (\"[epoch: %d] [acc_train: %f] [acc_test1: %f] [acc_test2: %f]\" % (epoch,acc_train, flag_acc_test1, flag_acc_test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-internet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
